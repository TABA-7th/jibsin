import time
import pandas as pd
import cv2
import json
from PIL import Image
import requests
import uuid
import time
import openai
import re
import base64
import numpy as np
from io import BytesIO
from PIL import Image
from dotenv import load_dotenv
import os
load_dotenv(r"C:\Users\senbo\Desktop\taba_project\.env")  # .env íŒŒì¼ ë¡œë“œ
api_key = os.getenv("OPENAI_API_KEY")
client_id=os.getenv("client_id")
client_secret=os.getenv("client_secret")
client = openai.OpenAI(api_key=api_key)
secret_key=os.getenv("secret_key")
api_url=os.getenv("api_url")

MODEL = "gpt-4o"
#ê³„ì•½ì„œì›ë³¸ì–‘ì‹
def base_xy():
    rows = [
        ['ë“±ê¸°ì‚¬í•­ì „ë¶€ì¦ëª…ì„œ',348,112,934,162],
        ['ì§‘í•©ê±´ë¬¼',520,166,766,216],
        ['[ì§‘í•©ê±´ë¬¼] ê±´ë¬¼ì£¼ì†Œ',26,298,908,346],
        ['[í‘œì œë¶€](1ë™ì˜ ê±´ë¬¼ì˜ í‘œì‹œ)',94,354,632,392],
        ['í‘œì‹œë²ˆí˜¸',34,406,130,444],
        ['ì ‘ìˆ˜',172,414,268,440],
        ['ì†Œì¬ì§€ë²ˆ, ê±´ë¬¼ ëª…ì¹­ ë° ë²ˆí˜¸', 318,410,590,440],
        ['([ë„ë¡œëª…ì£¼ì†Œ])',312,456,580,642],
        ['ê±´ë¬¼ë‚´ì—­',668,410,808,446],
        ['ë“±ê¸° ì›ì¸ ë° ê¸°íƒ€ì‚¬í•­',904,404,1140,448],
        ['ì—´ëŒì¼ì‹œ',22,1620,456,1656],
        ['(ëŒ€ì§€ê¶Œì´ ëª©ì ì¸ í† ì§€ì˜ í‘œì‹œ)',408,2456,788,2496],
        ['[í‘œì œë¶€] (ì „ìœ ë¶€ë¶„ì˜ ê±´ë¬¼ì˜ í‘œì‹œ)',80,2672,684,2720],
        ['í‘œì‹œë²ˆí˜¸',40,2740,130,2776],
        ['ì ‘ìˆ˜',166,2732,280,2776],
        ['ê±´ë¬¼ë²ˆí˜¸',322,2732,480,2780],
        ['(ê±´ë¬¼ë²ˆí˜¸)',316,2784,490,2842],
        ['ê±´ë¬¼ë‚´ì—­',522,2742,694,2770],
        ['(ê±´ë¬¼ë‚´ì—­)',506,2790,706,2850],
        ['ë“±ê¸°ì›ì¸ ë° ê¸°íƒ€ì‚¬í•­',806,2736,1064,2772],
        ['[ê°‘ êµ¬] (ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­)',86,3842,654,3898],
        ['ìˆœìœ„ë²ˆí˜¸',46,3908,134,3948], 
        ['ë“±ê¸°ëª©ì ',170,3910,314,3944],
        ['ì ‘ìˆ˜', 390,3904,490,3946],
        ['ë“±ê¸°ì›ì¸',524,3906,668,3952],
        ['ê´€ë¦¬ì ë° ê¸°íƒ€ì‚¬í•­', 824,3902,1030,3946],
        ['ì†Œìœ ì', 824,3902,1030,4462],
        ['[ì„ êµ¬] (ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ëŒ€í•œ ì‚¬í•­)', 88,4562,796,4608],
        ['ìˆœìœ„ë²ˆí˜¸',46,4628,134,4658],
        ['ë“±ê¸°ëª©ì ',170,4628,314,4658],
        ['ì ‘ìˆ˜', 390,4628,490,4658],
        ['ë“±ê¸°ì›ì¸',524,4628,668,4658],
        ['ê´€ë¦¬ì ë° ê¸°íƒ€ì‚¬í•­',824,4628,1030,4658],
        ['(ì±„ê¶Œìµœê³ ì•¡)',718,4662,1156,4752],
        ['ì´í•˜ì—¬ë°±',410,4952,689,4990]
    ]
    xy = pd.DataFrame(columns=['Text', 'x1', 'y1', 'x2', 'y2'])
    xy = pd.concat([xy, pd.DataFrame(rows, columns=xy.columns)], ignore_index=True)
    return xy

def merge_images(image_paths, output_path_2):
    # target_size = (1240, 1755)  # ì›í•˜ëŠ” ì´ë¯¸ì§€ í¬ê¸°

    # # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì™€ í¬ê¸° ì¡°ì •
    # images = []
    # for img_path in image_paths:
    #     image = cv2.imread(img_path)  # OpenCVë¡œ ì´ë¯¸ì§€ ì½ê¸°
    #     resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)  # í¬ê¸° ì¡°ì •
    #     images.append(Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)))  # OpenCV â†’ PIL ë³€í™˜
    
    images = []
    for img_path in image_paths:
        image = cv2.imread(img_path)  # OpenCVë¡œ ì´ë¯¸ì§€ ì½ê¸°
        images.append(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)))  # OpenCV â†’ PIL ë³€í™˜

    total_height = sum(img.height for img in images)
    max_width = max(img.width for img in images)

    merged_image = Image.new("RGB", (max_width, total_height))

    # ì´ë¯¸ì§€ ë¶™ì´ê¸°
    y_offset = 0
    for img in images:
        merged_image.paste(img, (0, y_offset))
        y_offset += img.height
    
    # ë³‘í•©ëœ ì´ë¯¸ì§€ ì €ì¥
    merged_image.save(output_path_2)
    
    return merged_image

def cre_ocr(image_path):
    # target_size = (1240, 1753)
    image = cv2.imread(image_path)
    # image_1 = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)
    request_json = {
        'images': [{'format': 'jpg', 'name': 'demo'}],
        'requestId': str(uuid.uuid4()),
        'version': 'V2',
        'timestamp': int(round(time.time() * 1000))
    }
    payload = {'message': json.dumps(request_json).encode('UTF-8')}
    headers = {'X-OCR-SECRET': secret_key}

    # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ ì „ì†¡
    _, img_encoded = cv2.imencode('.jpg', image)
    files = [('file', ('image.jpg', img_encoded.tobytes(), 'image/jpeg'))]

    response = requests.request("POST", api_url, headers=headers, data=payload, files=files)
 
    #ê°€ì¥ ê¸°ë³¸ì ì¸ íŒŒì¼ url ë¶ˆëŸ¬ì™€ì„œ ë³´ë‚´ëŠ” ë°©ì‹
    payload = {'message': json.dumps(request_json).encode('UTF-8')}
    files = [('file', open(image_path, 'rb'))]
    headers = {'X-OCR-SECRET': secret_key}
    response = requests.post(api_url, headers=headers, data=payload, files=files)
    if response.status_code == 200:
        ocr_results = response.json()

        all_data = []
        for image_result in ocr_results['images']:
            for field in image_result['fields']:
                text = field['inferText']
                bounding_box = field['boundingPoly']['vertices']
                x1, y1 = int(bounding_box[0]['x']), int(bounding_box[0]['y'])
                x2, y2 = int(bounding_box[2]['x']), int(bounding_box[2]['y'])
                all_data.append({
                    "Text": text,
                    "x1": x1, "y1": y1,
                    "x2": x2, "y2": y2
                })
        df = pd.DataFrame(all_data)
        return df

#-----------[ì¶”ê°€í•¨ìˆ˜]-------------
def get_image_height(image_path):
    with Image.open(image_path) as img:
        return img.height  # ì´ë¯¸ì§€ ë†’ì´ ë°˜í™˜
def organize_by_pages(data, image_paths):
    # ì´ë¯¸ì§€ ë†’ì´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    page_heights = [get_image_height(img) for img in image_paths]
    
    # í˜ì´ì§€ ê²½ê³„ ê³„ì‚°
    page_boundaries = []
    current_height = 0
    for height in page_heights:
        page_boundaries.append({
            'start': current_height,
            'end': current_height + height
        })
        current_height += height

    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™” (í‚¤ë¥¼ "1í˜ì´ì§€" í˜•ì‹ìœ¼ë¡œ ìƒì„±)
    result = {f"{i+1}í˜ì´ì§€": {} for i in range(len(page_heights))}
    
    # ê° í•­ëª©ì„ í•´ë‹¹í•˜ëŠ” í˜ì´ì§€ì— í• ë‹¹
    for key, value in data.items():
        y1 = value['bounding_box']['y1']
        
        # y1 ê°’ì´ ì–´ëŠ í˜ì´ì§€ ë²”ìœ„ì— ì†í•˜ëŠ”ì§€ í™•ì¸
        for page_num, boundary in enumerate(page_boundaries):
            if boundary['start'] <= y1 < boundary['end']:
                # í•´ë‹¹ í˜ì´ì§€ì— í•­ëª© ì¶”ê°€ (í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘)
                page_key = f"{page_num+1}í˜ì´ì§€"
                new_value = value.copy()
                # y ì¢Œí‘œ ì¡°ì • (í˜ì´ì§€ ì‹œì‘ì ë§Œí¼ ë¹¼ê¸°)
                new_value['bounding_box']['y1'] -= boundary['start']
                new_value['bounding_box']['y2'] -= boundary['start']
                result[page_key][key] = new_value
                break
    
    return result
#ìˆ˜ì •
def read_ocr(image_paths, out_path):
# ì´ë¯¸ì§€ ë†’ì´ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ í™œìš©)
    page_height = [get_image_height(img) for img in image_paths]
    all_dfs = []
    y = 0
    for j, image_path in enumerate(image_paths):
        df = cre_ocr(image_path)
        df[["y1", "y2"]] += y  # y ê°’ ì¼ê´„ ë³€ê²½
        all_dfs.append(df)
        y += page_height[j]  # y ê°’ ì—…ë°ì´íŠ¸

    merged_df = pd.concat(all_dfs, ignore_index=True)
    # merged_df.to_csv(r'C:\Users\senbo\Desktop\taba\python\rrr\test_3.csv')
    # merged_df=pd.read_csv(r'C:\Users\senbo\Desktop\taba\python\rrr\test_3.csv')

    xy=base_xy()
    xy_json = xy.to_json(orient="records", force_ascii=False)
    df_json = merged_df.to_json(orient="records", force_ascii=False)

    target_texts = {
            "ì¢…ë¥˜": "ë“±ë³¸ ì¢…ë¥˜ (ì§‘í•©ê±´ë¬¼, ê±´ë¬¼, í† ì§€ ì¤‘ í•˜ë‚˜)",
            "(ê±´ë¬¼ì£¼ì†Œ)": "[ë“±ë³¸ì¢…ë¥˜] ë„ë¡œëª… ì£¼ì†Œ (ì˜ˆ: [ì§‘í•©ê±´ë¬¼] ì •ì™•ëŒ€ë¡œ 53ë²ˆê¸¸ 29)",
            "ì—´ëŒì¼ì‹œ": "yyyyë…„ mmì›” ddì¼ hhì‹œmmë¶„ssì´ˆ",
            "(ê°‘êµ¬)":"í…ìŠ¤íŠ¸",
            "(ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­)": "(ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­)",
            "ì†Œìœ ì":"ì´ë¦„",
            "ì‹ íƒ":"ì‹ íƒ (ì˜ˆ: ì‹ íƒ, ì´ì™¸ì˜ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì•ˆë¨)",
            "ì••ë¥˜":"ì••ë¥˜ (ì˜ˆ: ì••ë¥˜, ì´ì™¸ì˜ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì•ˆë¨)",
            "ê°€ì²˜ë¶„":"ê°€ì²˜ë¶„ (ì˜ˆ: ê°€ì²˜ë¶„, ì´ì™¸ì˜ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì•ˆë¨)",
            "ê°€ì••ë¥˜":"ê°€ì••ë¥˜ (ì˜ˆ: ê°€ì••ë¥˜, ì´ì™¸ì˜ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì•ˆë¨)",
            "(ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ëŒ€í•œ ì‚¬í•­)":"(ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ëŒ€í•œ ì‚¬í•­)",
            "(ì±„ê¶Œìµœê³ ì•¡)": "ìµœê³ ì±„ê¶Œì•¡ ê¸ˆ ###ì›(ì˜ˆ: ì±„ê¶Œìµœê³ ì•¡ ê¸ˆ1,000,000,000ì›)",
            "ì´í•˜ì—¬ë°±": "ì´ í•˜ ì—¬ ë°±"
        }

    # OpenAI API ìš”ì²­
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            f"ë‹¤ìŒì€ OCR ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ì…ë‹ˆë‹¤.\n\n"
                            f"**ìœ„ì¹˜ ë°ì´í„° (xy):**\n{xy_json}\n\n"
                            f"**ë‚´ìš© ë°ì´í„° (df):**\n{df_json}\n\n"
                            f"**ì‘ì—… ëª©í‘œ:**\n"
                            f"- ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'NA'ë¡œ í‘œì‹œ\n\n"
                            f"- `xy` ë°ì´í„°ì˜ ìœ„ì¹˜ ì •ë³´(ì¢Œí‘œ)ë¥¼ í™œìš©í•˜ì—¬ `df` ë°ì´í„°ì™€ ë§¤ì¹­. {xy_json}ì˜ ìœ„ì¹˜ëŠ” ì°¸ê³ ë§Œí•˜ê³  í•­ìƒ {df_json}ì„ ë”°ë¥¸ë‹¤.\n"
                            f"- 'xy' ë°ì´í„°ì˜ ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸°ëŠ” 'df'ì— ë§ê²Œ ì¡°ì •ëœë‹¤"
                            f" **ê° í•­ëª©ì˜ ì¶œë ¥ í˜•ì‹:**\n"
                            + "\n".join([f"- **{key}**: {value}" for key, value in target_texts.items()]) +
                            f"\n\n**ê²°ê³¼ í˜•ì‹:**\n"
                            f"- JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ê° í•­ëª©ì˜ ë°”ìš´ë”© ë°•ìŠ¤ í¬í•¨)\n"
                            f"- **ì¶œë ¥ ë°ì´í„°ê°€ ì§€ì •ëœ í˜•ì‹ê³¼ ë‹¤ë¥¼ ê²½ìš° ìë™ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜**\n\n"
                            f"**ë°˜í™˜ ì˜ˆì‹œ:**\n"
                            f"{{\n"
                            f"  \"ì¢…ë¥˜\": {{\"text\": \"ì§‘í•©ê±´ë¬¼\", \"bounding_box\": {{\"x1\": 100, \"y1\": 200, \"x2\": 300, \"y2\": 250}}}},\n"
                            f"  \"ê±´ë¬¼ì£¼ì†Œ\": {{\"text\": \"ì •ì™•ëŒ€ë¡œ 53ë²ˆê¸¸ 29\", \"bounding_box\": {{\"x1\": 120, \"y1\": 220, \"x2\": 320, \"y2\": 270}}}},\n"
                            f"  \"(ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­)\": {{\"text\": \"( ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­ )\", \"bounding_box\": {{\"x1\": 120, \"y1\": 220, \"x2\": 320, \"y2\": 270}}}},\n"
                            f"  \"ì†Œìœ ì£¼\": {{\"text\": \"( ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­ )\", \"bounding_box\": {{\"x1\": 120, \"y1\": 220, \"x2\": 320, \"y2\": 270}}}},\n"
                            f"  \"(ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ê´€í•œ ì‚¬í•­)\": {{\"text\": \"(ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ê´€í•œ ì‚¬í•­)\", \"bounding_box\": {{\"x1\": 120, \"y1\": 220, \"x2\": 320, \"y2\": 270}}}},\n"                            
                            f"  \"ì—´ëŒì¼ì‹œ\": {{\"text\": \"2025ë…„ 02ì›” 15ì¼ 14ì‹œ 48ë¶„\", \"bounding_box\": {{\"x1\": 150, \"y1\": 250, \"x2\": 350, \"y2\": 300}}}},\n"
                            f"  \"ì±„ê¶Œìµœê³ ì•¡\": {{\"text\": \"ì±„ê¶Œìµœê³ ì•¡ ê¸ˆ1,000,000,000ì›\", \"bounding_box\": {{\"x1\": 170, \"y1\": 270, \"x2\": 370, \"y2\": 320}}}}\n"
                            f"}}\n\n"
                            f"**ì£¼ì˜ì‚¬í•­:**\n"
                            f"- ëª¨ë“  ì¢Œí‘œëŠ” dfë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¶œë ¥í•œë‹¤."
                            f"- dfë¥¼ í•­ìƒ ìš°ì„ ì‹œí•œë‹¤."
                            f"- í…ìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ë°”ìš´ë”© ë°•ìŠ¤ì— ê±¸ì³ ìˆëŠ” ê²½ìš°, ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨\n"
                            f"- ë‚´ìš©ì´ ì—†ì„ ê²½ìš° `NA`ë¡œ ë°˜í™˜, text ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ì¢Œí‘œë¥¼ 0, 0, 0, 0ìœ¼ë¡œ í•´ì¤˜.\n"
                            f"- df ê¸°ì¤€ìœ¼ë¡œ ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ê²ƒ"
                            f"- ì†Œìœ ì£¼ëŠ” '(ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ê´€í•œ ì‚¬í•­)'ì™€ '(ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­) ì‚¬ì´ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì´ë¦„ì´ë‹¤'"
                            f"- ì†Œìœ ì£¼ê°€ ì—¬ëŸ¬ëª…ì¸ ê²½ìš° ì†Œìœ ì£¼_1, ì†Œìœ ì£¼_2 ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ëœë‹¤"
                            f"- ì±„ê¶Œìµœê³ ì•¡ì€ '(ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­)' ê³¼ 'ì´í•˜ì—¬ë°±' ì‚¬ì´ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ê¸ˆì•¡ì´ë‹¤."
                            f"- ì±„ê¶Œìµœê³ ì•¡ì€ ì—¬ëŸ¬ê°œì¸ ê²½ìš° ì±„ê¶Œìµœê³ ì•¡_1, ì±„ê¶Œìµœê³ ì•¡_2ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ëœë‹¤."
                            f"- ì±„ê¶Œìµœê³ ì•¡ì€ ì±„ê¶Œìµœê³ ì•¡_i ì¤‘ ê°€ì¥ iê°€ í° ê²ƒë§Œì„ ì¶œë ¥í•œë‹¤."
                            f"- JSON í˜•ì‹ì´ ì •í™•í•˜ë„ë¡ ë°˜í™˜í•  ê²ƒ!\n"
                            f"- JSON í˜•ì‹ ì´ì™¸ì˜ ì–´ë–¤ ì•Œë¦¼, ë‚´ìš©ì€ ì²¨ê°€í•˜ì§€ ë§ê²ƒ!\n"
                            f"- ë°˜í™˜ ë‚´ìš© ì™¸ì˜ ê²½ê³ , ì•Œë¦¼ì€ ë°˜í™˜í•˜ì§€ ë§ê²ƒ\n"
                            f" 'ì•„ë˜ëŠ” ì œê³µëœ `xy` ë° `df` ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í•­ëª©ì„ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤'ì™€ ê°™ì€ ì•Œë¦¼ì€ ì ˆëŒ€ ê¸ˆì§€\n"
                            f" OpenAI ì‘ë‹µë‚´ìš©ê¸ˆì§€\n"

                        )
                    }
                ]
            }
        ],
        max_tokens=5000,
        temperature=0.2,
        top_p=1.0
    )

    # ì‘ë‹µ ì²˜ë¦¬
    text = response.choices[0].message.content.strip()
    text = fix_json_format(text)
    data=json.loads(text)


    # "(ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­)"ê³¼ "(ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ê´€í•œ ì‚¬í•­)"ì„ ì‚­ì œ
    data.pop("(ì†Œìœ ê¶Œì— ê´€í•œ ì‚¬í•­)", None)
    data.pop("(ì†Œìœ ê¶Œ ì´ì™¸ì˜ ê¶Œë¦¬ì— ê´€í•œ ì‚¬í•­)", None)

    

    return ttj(json.dumps(data, ensure_ascii=False, indent=4), out_path)
#--------------------------------------------
def fix_json_format(text: str) -> str:
    """JSON í˜•ì‹ ì˜¤ë¥˜ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” í•¨ìˆ˜."""
    text = text.strip()

    # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
    text = text.replace("```json", "").replace("```", "").strip()

    # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸(ì„¤ëª… ë“±) ìë™ ì œê±°
    json_end_index = text.rfind("}")
    if json_end_index != -1:
        text = text[:json_end_index+1]  # JSON ë¶€ë¶„ë§Œ ë‚¨ê¹€

    # JSONì—ì„œ ëˆ„ë½ëœ ',' ìë™ ì¶”ê°€
    text = re.sub(r'}\s*{', '}, {', text)

    # JSON ë‚´ ìˆ«ì ì‰¼í‘œ ì˜¤ë¥˜ ìˆ˜ì • (100000,000ì› -> 100,000,000ì›)
    text = re.sub(r'(\d{1,3})(\d{3},\d{3})', r'\1,\2', text)

    return text

#ttj í•¨ìˆ˜ì— ì €ì¥ ê¸°ëŠ¥ ì œê±°
def ttj(text: str, output_file: str) -> str:
    """OCR ê²°ê³¼ JSON ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜."""
    try:
        text = fix_json_format(text)  # JSON í˜•ì‹ ìë™ ìˆ˜ì •
        data = json.loads(text)  # JSON ë³€í™˜

        def fix_text(value):
            if value == "NA":
                return value
            value = re.sub(r'(\d+)\s+(\d+)', r'\1,\2', value)  # ìˆ«ì ì‚¬ì´ ê³µë°±ì„ ì½¤ë§ˆë¡œ ë³€í™˜
            return value.strip()

        # ëª¨ë“  JSON í•„ë“œì˜ ê°’ ìë™ ìˆ˜ì •
        for key, value in data.items():
            if isinstance(value, dict) and "text" in value:
                value["text"] = fix_text(value["text"])

        return data

    except json.JSONDecodeError as e:
        print(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
        print("ğŸ“Œ ì˜¤ë¥˜ ë°œìƒ JSON ë‚´ìš©:\n", text)  # JSON ë””ë²„ê¹… ì¶œë ¥
        return f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}"

def request(img_list,output_path, output_path_2):
# def request():
    # img_list = [rf"C:\Users\senbo\Desktop\taba\python\reg\000{i}.jpg" for i in range(1, 4)]
    # output_path = rf"C:\Users\senbo\Desktop\taba\python\rrr\test_bui_1.json"
    # output_path_2 = r"C:\Users\senbo\Desktop\taba\python\rrr\test_3_1.jpg"
    
    merge_images(img_list,output_path_2)
    data = read_ocr(img_list,output_path)

    organized_data = organize_by_pages(data, img_list)

    # ê²°ê³¼ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(organized_data, f, ensure_ascii=False, indent=2)

# request()
